{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e87e5tNx5aMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ee091fa-1ee3-4de0-dbf6-a51170d87326"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "%pip install python-dotenv --upgrade --quiet langchain langchain-huggingface sentence-transformers langchain-community\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# Using a FREE, open-source model from Hugging Face\n",
        "# 'all-MiniLM-L6-v2' is small, fast, and very good for English.\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector = embeddings.embed_query(\"Shoes\")\n",
        "\n",
        "print(f\"Dimensionality: {len(vector)}\")\n",
        "print(f\"First 5 numbers: {vector[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wZTkVm56BTK",
        "outputId": "dda1c689-a424-4ad7-acf4-d674b6a747d4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensionality: 384\n",
            "First 5 numbers: [-0.08814936131238937, 0.03152328357100487, 0.0299678985029459, 0.028904153034090996, 0.0034591611474752426]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "vec_cat = embeddings.embed_query(\"Cat\")\n",
        "vec_dog = embeddings.embed_query(\"Dog\")\n",
        "vec_car = embeddings.embed_query(\"Car\")\n",
        "\n",
        "print(f\"Cat vs Dog: {cosine_similarity(vec_cat, vec_dog):.4f}\")\n",
        "print(f\"Cat vs Car: {cosine_similarity(vec_cat, vec_car):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhBUdwbD6Vrh",
        "outputId": "6e873cdd-f1b7-490b-d512-5f4fcd94458e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cat vs Dog: 0.6606\n",
            "Cat vs Car: 0.4633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "%pip install python-dotenv --upgrade --quiet faiss-cpu langchain-huggingface sentence-transformers langchain-community langchain-google-genai\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "# Using the same free model as Part 4a\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WfuIPTB6Zb9",
        "outputId": "23379bcb-7f9b-4c7f-e19f-688206a6ee8b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Google API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "docs = [\n",
        "    Document(page_content=\"Satwik's favorite food is Butter Chicken with naan.\"),\n",
        "    Document(page_content=\"The secret password to the time machine is 'Time'.\"),\n",
        "    Document(page_content=\"LangChain is a framework for developing applications powered by language models.\"),\n",
        "]"
      ],
      "metadata": {
        "id": "ARTfE2Ms6dLl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "toicBkEV7HCn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "template = \"\"\"\n",
        "Answer based ONLY on the context below:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "result = chain.invoke(\"What is his favorite food\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j5vwWfQ7JrK",
        "outputId": "0764d5aa-1a06-4156-c729-5230b3ad4b3f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "His favorite food is Butter Chicken with naan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Mock Data: 10,000 vectors of size 128\n",
        "d = 128\n",
        "nb = 10000\n",
        "xb = np.random.random((nb, d)).astype('float32')"
      ],
      "metadata": {
        "id": "cY5FCrZM7NEy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = faiss.IndexFlatL2(d)\n",
        "index.add(xb)\n",
        "print(f\"Flat Index contains {index.ntotal} vectors\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfODeyIg7V3q",
        "outputId": "cf5c3564-c907-49fd-966b-75a5d913fccf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flat Index contains 10000 vectors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlist = 100 # How many 'zip codes' (clusters) we want\n",
        "quantizer = faiss.IndexFlatL2(d) # The calculator for distance\n",
        "index_ivf = faiss.IndexIVFFlat(quantizer, d, nlist)\n",
        "\n",
        "# We MUST train it first so it learns where the clusters are\n",
        "index_ivf.train(xb)\n",
        "index_ivf.add(xb)"
      ],
      "metadata": {
        "id": "zXFpVJL87X3C"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "M = 16 # Number of connections per node (The 'Hub' factor)\n",
        "index_hnsw = faiss.IndexHNSWFlat(d, M)\n",
        "index_hnsw.add(xb)"
      ],
      "metadata": {
        "id": "4-TTg7pE7a3w"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = 8 # Split vector into 8 sub-vectors\n",
        "index_pq = faiss.IndexPQ(d, m, 8)\n",
        "index_pq.train(xb)\n",
        "index_pq.add(xb)\n",
        "print(\"PQ Compression complete. RAM usage minimized.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8CVxErl7dNB",
        "outputId": "562877e7-1ade-4ec9-99f4-210a7e28a68d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PQ Compression complete. RAM usage minimized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uex0rgjI7gUj"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}